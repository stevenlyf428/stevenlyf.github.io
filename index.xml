<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>小飞的学习屋</title>
    <link>http://stevenlyf428.github.io/</link>
    <description>Recent content on 小飞的学习屋</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Nov 2021 20:35:07 +0800</lastBuildDate><atom:link href="http://stevenlyf428.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>IterE</title>
      <link>http://stevenlyf428.github.io/posts/itere/</link>
      <pubDate>Sun, 28 Nov 2021 20:35:07 +0800</pubDate>
      
      <guid>http://stevenlyf428.github.io/posts/itere/</guid>
      <description>[toc]
Reserach problem   问题一：传统的KGE(Knowledge Graph Embedding)存在稀疏性问题(对于训练集中稀疏的实体，KGE模型学不到很好的表示)；
  问题二：在KG中做rule learning具有效率的问题(搜索空间巨大)。
  针对以上两点，作者思考是否有一种方式能够同时学习embedding和rule，同时让彼此相互促进。针对问题一，作者利用rules去帮助更好的学习embedding；针对问题二，作者采取一种搜索加剪枝的迭代式学习的策略。
符号说明：
 本文的rule learning，即Axiom，形如$(x, r, z) \gets (x, r, y), (y, r, z)$，以下统称为Axiom； Knowledge Graph统称KG。  Flowchart Flowchart 1：
Flowchart 2:Proposed algorithm 一共有三个模块，分为：
 Part 1:Embedding learning，负责学习三元组的Embedding表示。利用原始的训练集、规则生成的新三元组(作为补充的正例)、和随机负采样出来的反例，进行训练； Part 2:Axiom Induction，负责生成Axiom pool（有利于减少搜索空间，仅生成一次）和筛选高置信度的Axiom； Part 3:Axiom Injection，负责筛选出稀疏的实体结点，利用Axiom为稀疏的实体生成新的三元组。  最后，对这三个模块迭代式学习n次后获得最终的KG Embedding表示。
Embedding Learning KG Embedding，是利用原始的训练集、规则生成的新三元组(作为补充的正例)、和随机负采样出来的反例，进行训练。
首先，是明确模型的训练集： $$ \mathcal{I}=\left{\left((s, r, o), l_{\text {sro }}\right) \mid((s, r, o)) \in \mathcal{T} \wedge \mathcal{T}_{\text {axiom }} \wedge \mathcal{T}_{\text {negative }}\right} $$ 其中，</description>
    </item>
    
    <item>
      <title>关于我</title>
      <link>http://stevenlyf428.github.io/about/</link>
      <pubDate>Thu, 19 Aug 2021 02:56:51 +0800</pubDate>
      
      <guid>http://stevenlyf428.github.io/about/</guid>
      <description>hi👋，我叫stevenlyf。各大平台都叫stevenlyf428。
关于名字 至于为什么叫这个名字，是因为小学时候英语老师给每个同学起名，你叫Tom，你叫Jerry&amp;hellip;轮到我，老师说你就叫steven吧。😓
至于后面还带了个lyf，这是我中文名字的缩写。
有的时候你还会看见我叫做stevenlyf428。428是我大学时候的宿舍号。
至于为什么要加上这么长这么怪的后缀，那是因为网络水太深，stevenlyf在好多平台都重名了QAQ（这都能重！555555）。。。。
关于我 目前研一，在西南大学的lab1010实验室研究自然语言处理（NLP）🏫。
PS  2022年2月23日，小飞同学准备跑路后端开发，开始学go；  </description>
    </item>
    
    <item>
      <title>设计人生</title>
      <link>http://stevenlyf428.github.io/posts/%E8%AE%BE%E8%AE%A1%E4%BA%BA%E7%94%9F/</link>
      <pubDate>Thu, 19 Aug 2021 02:56:24 +0800</pubDate>
      
      <guid>http://stevenlyf428.github.io/posts/%E8%AE%BE%E8%AE%A1%E4%BA%BA%E7%94%9F/</guid>
      <description>小盆友,你们是不是有很多问号们，总是怀疑自己，觉得自己好惨一人。
是不是有时候突然发现自己的人生轨迹总是莫名开错方向。
来，我来开导开导你。
1、悲惨的开始 比如我，可怜的娃子，作为双非子弟，一战心心念念的华师大。当初复习的时候极其自信，总是觉得分不在高，够用就好。于是开始复习的时间很晚，复习也是吊儿郎当的。于是，命运开始折磨我，最终连复试也没进去真的差了一丢丢🤏。
想死的心都有了，后悔当初不多再努力一点，后悔当初不如报个好的学校。
那个难受啊！！！
2、假设这次十分好运 于是我在床上开始了吟唱和思考，为什么会打出来吟唱。
我给自己设计，假如自己高出复试线，高出2-3分，那自己是必然的十分开心，雄赳赳气昂昂的开始准备复试。
那复习起来叫做一个气吞山河，脚踢北斗。
3、接下来呢 悲惨的是，自己固然准备面试的还不错。
但是，我面试有点小紧张，我拿出我唯一拿的出手的美赛M奖参与奖。
面前的5个导师纷纷询问，你的文本是如何处理的呀？我说，将文本转换成词向量，放入贝叶斯分类器中。
哦？那你的文本向量维度过高如何处理？啊啊啊没想过，文本相对来说还不算高，处理起来还能接受。
你只用过机器学习的方法么，还用过别的方法的处理么？我当时才大三上呀，我知道可以用循环神经网络和长短期记忆网络处理可能更高，但是我没试过。也用不到那种呀
（省略5分钟）
最后导师评价，你做的还都很浅。我：放过我吧
最后的最后复试挂了。
4、假设再次十分好运 好家伙，我压线进了复试，复试中又逆袭了。
我，得知消息后，和爸妈拥抱痛哭。
赶紧联系导师，发现最喜欢的吴老师导师没名额了了，啊啊啊啊，可是我就奔着他来的，那好吧，换个导师，大不了导师不好的话，我就出来就业，科研梦就继续成为梦吧。
5、总结 我们可以接着这条路继续设想，可是我们会发现总会有一些不爽的事情发生。
比如，压线进了复试，复试被刷。
成功被录取，发现没好导师带。
有好导师带，但是自己实力不足，时常自闭。
自己成功熬到毕业，发现没能找到好工作。
等等等等，诸如此类，我们在遇到挫折面前，可以给自己设想一下去改变一下命运。 假如没有这个的困难，自己一定会更顺利么？
恐怕不是那么简单，我们总会遇到一个接着一个的困难，哪怕相比于现在的自己要更幸运。
6、感悟 还是要活在当下。
真正的勇士是要敢于直面困难。
困难无穷多也，我们遇到挫折，可以伤心难受，但是不要气馁，我们需要的是利用这次的挫折去反思，去避免以后的更多困难。
这才是真正的捷径。</description>
    </item>
    
    
    
  </channel>
</rss>
